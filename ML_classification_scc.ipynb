{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "source": [
    "import os\r\n",
    "import csv\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "from wordcloud import WordCloud,STOPWORDS\r\n",
    "import nltk\r\n",
    "from nltk.stem.snowball import SnowballStemmer\r\n",
    "import re\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.pipeline import Pipeline\r\n",
    "from sklearn.multiclass import OneVsRestClassifier\r\n",
    "from skmultilearn.problem_transform import ClassifierChain\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\r\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\r\n",
    "from ekphrasis.dicts.emoticons import emoticons\r\n",
    "import spacy\r\n",
    "nlp1 = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\r\n",
    "from spacy.lang.en import English\r\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "source": [
    "\r\n",
    "training_dataset =\"/Users/manis/Dropbox/research/2021/lrev/notebooks/train_scc.csv\"\r\n",
    "training_data = pd.read_csv(training_dataset)\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "source": [
    "\r\n",
    "print(\"starting setup\")\r\n",
    "ekphrasis_process = TextPreProcessor(\r\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\r\n",
    "        'time', 'url', 'date', 'number'],\r\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\r\n",
    "        'emphasis', 'censored'},\r\n",
    "    fix_html=True, \r\n",
    "    segmenter=\"twitter\", \r\n",
    "    corrector=\"twitter\", \r\n",
    "    unpack_hashtags=True,  \r\n",
    "    unpack_contractions=True, \r\n",
    "    spell_correct_elong=False, \r\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\r\n",
    "    dicts=[emoticons]\r\n",
    ")\r\n",
    "\r\n",
    "print(\"finished setup\")\r\n",
    "\r\n",
    "\r\n",
    "def ekphrasis_word(word):\r\n",
    "\r\n",
    "    return(\" \".join(ekphrasis_process.pre_process_doc(word)))\r\n",
    "\r\n",
    "\r\n",
    "def get_ekphrasis(word):\r\n",
    "    import re\r\n",
    "    b = ekphrasis_word(word)\r\n",
    "    TAG_RE = re.compile(r'<[^>]+>')\r\n",
    "    res = TAG_RE.sub('',b)\r\n",
    "    res = res.rstrip().lstrip()\r\n",
    "    return res    \r\n",
    "\r\n",
    "\r\n",
    "def html_prepr(text):\r\n",
    "    get_text = re.compile('<.*?>')\r\n",
    "    res = re.sub(get_text, ' ', str(text))\r\n",
    "    return res\r\n",
    "\r\n",
    "\r\n",
    "def alphanum_cl(text): \r\n",
    "    processed = re.sub(r'[?|!|\\'|\"|#]',r'',text)\r\n",
    "    processed = re.sub(r'[.|,|)|(|\\|/]',r' ',processed)\r\n",
    "    processed = processed.strip()\r\n",
    "    processed = processed.replace(\"\\n\",\" \")\r\n",
    "    return processed\r\n",
    "\r\n",
    "\r\n",
    "def alphanum_pr(text):\r\n",
    "    res_word = \"\"\r\n",
    "    for word in text.split():\r\n",
    "        get_word = re.sub('[^a-z A-Z]+', ' ', word)\r\n",
    "        res_word += get_word\r\n",
    "        res_word += \" \"\r\n",
    "    res_word = res_word.strip()\r\n",
    "    return res_word\r\n",
    "\r\n",
    "def spacy_lemmatizer(text):\r\n",
    "    doc = nlp1(text)\r\n",
    "    return (\" \".join([token.lemma_ for token in doc]))\r\n",
    "\r\n",
    "training_data['tweet_txt'] = training_data['tweet_txt'].str.lower()\r\n",
    "training_data['tweet_txt'] = training_data['tweet_txt'].apply(html_prepr)\r\n",
    "training_data['tweet_txt'] = training_data['tweet_txt'].apply(alphanum_cl)\r\n",
    "training_data['tweet_txt'] = training_data['tweet_txt'].apply(alphanum_pr)\r\n",
    "training_data['tweet_txt'] = training_data['tweet_txt'].apply(get_ekphrasis)\r\n",
    "training_data['tweet_txt'] = training_data['tweet_txt'].apply(spacy_lemmatizer)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "starting setup\n",
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n",
      "finished setup\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "source": [
    "get_stopwords = set(stopwords.words('english'))\r\n",
    "get_stopwords.update(['zero','one','two','three','four','five','six','seven','eight','nine','ten','may','also','across','among','beside','however','yet','within'])\r\n",
    "re_get_stopwords = re.compile(r\"\\b(\" + \"|\".join(get_stopwords) + \")\\\\W\", re.I)\r\n",
    "def delsw(sentence):\r\n",
    "    global re_get_stopwords\r\n",
    "    return re_get_stopwords.sub(\" \", sentence)\r\n",
    "\r\n",
    "training_data['tweet_txt'] = training_data['tweet_txt'].apply(removeStopWords)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "source": [
    "ps  = SnowballStemmer(\"english\")\r\n",
    "def stemming(sentence):\r\n",
    "    stemres = \"\"\r\n",
    "    for word in sentence.split():\r\n",
    "        stem = ps.stem(word)\r\n",
    "        stemres += stem\r\n",
    "        stemres += \" \"\r\n",
    "    stemres = stemres.strip()\r\n",
    "    return stemres\r\n",
    "\r\n",
    "# training_data['tweet_txt'] = training_data['tweet_txt'].apply(stemming)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "source": [
    "train, test = train_test_split(training_data, random_state=42, test_size=0.20, shuffle=True)\r\n",
    "traintext = train['tweet_txt']\r\n",
    "testtext = test['tweet_txt']\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3), norm='l2')\r\n",
    "vectorizer.fit(traintext)\r\n",
    "vectorizer.fit(testtext)\r\n",
    "x_train = vectorizer.transform(traintext)\r\n",
    "y_train = train.drop(labels = ['tweet_txt'], axis=1)\r\n",
    "x_test = vectorizer.transform(testtext)\r\n",
    "y_test = test.drop(labels = ['tweet_txt'], axis=1)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "source": [
    "\r\n",
    "classifier = ClassifierChain(LogisticRegression())\r\n",
    "classifier.fit(x_train, y_train)\r\n",
    "classifier_predict = classifier.predict(x_test)\r\n"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "0993227984ad8824c264444a38e824e4c951a7592d518f6e93ecdb2b05867cef"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}