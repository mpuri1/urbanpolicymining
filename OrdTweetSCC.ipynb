{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "\r\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\r\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\r\n",
    "from ekphrasis.dicts.emoticons import emoticons\r\n",
    "import spacy\r\n",
    "nlp1 = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\r\n",
    "from spacy.lang.en import English\r\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\r\n",
    "import nltk\r\n",
    "from nltk.stem import PorterStemmer\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from nltk.stem.snowball import SnowballStemmer\r\n",
    "import re\r\n",
    "import sys\r\n",
    "import warnings\r\n",
    "import tkinter as tk\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "ps  = SnowballStemmer(\"english\")\r\n",
    "from tkinter import messagebox\r\n",
    "from nltk.stem import WordNetLemmatizer\r\n",
    "wnl = WordNetLemmatizer()\r\n",
    "import pickle\r\n",
    "\r\n",
    "\r\n",
    "print(\"starting setup\")\r\n",
    "ekphrasis_process = TextPreProcessor(\r\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\r\n",
    "        'time', 'url', 'date', 'number'],\r\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\r\n",
    "        'emphasis', 'censored'},\r\n",
    "    fix_html=True, \r\n",
    "    segmenter=\"twitter\", \r\n",
    "    corrector=\"twitter\", \r\n",
    "    unpack_hashtags=True,  \r\n",
    "    unpack_contractions=True, \r\n",
    "    spell_correct_elong=False, \r\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\r\n",
    "    dicts=[emoticons]\r\n",
    ")\r\n",
    "\r\n",
    "print(\"finished setup\")\r\n",
    "\r\n",
    "\r\n",
    "def ekphrasis_word(word):\r\n",
    "\r\n",
    "    return(\" \".join(ekphrasis_process.pre_process_doc(word)))\r\n",
    "\r\n",
    "\r\n",
    "def get_ekphrasis(word):\r\n",
    "    import re\r\n",
    "    b = ekphrasis_word(word)\r\n",
    "    TAG_RE = re.compile(r'<[^>]+>')\r\n",
    "    res = TAG_RE.sub('',b)\r\n",
    "    res = res.rstrip().lstrip()\r\n",
    "    return res    \r\n",
    "\r\n",
    "def spacy_lemmatizer(text):\r\n",
    "    doc = nlp1(text)\r\n",
    "    return (\" \".join([token.lemma_ for token in doc]))\r\n",
    "\r\n",
    "ps = PorterStemmer()\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "starting setup\n",
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n",
      "finished setup\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def remove_stopwords(text):\r\n",
    "    \r\n",
    "    nlp = English()\r\n",
    "    my_doc = nlp(text)\r\n",
    "    # Create list of word tokens\r\n",
    "    token_list = []\r\n",
    "    for token in my_doc:\r\n",
    "        token_list.append(token.text)\r\n",
    "    filtered_sentence =[] \r\n",
    "\r\n",
    "    for word in token_list:\r\n",
    "        lexeme = nlp.vocab[word]\r\n",
    "        if lexeme.is_stop == False:\r\n",
    "            filtered_sentence.append(word) \r\n",
    "    res = \" \".join(x for x in filtered_sentence)   \r\n",
    "    return res\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "\r\n",
    "if not sys.warnoptions:\r\n",
    "    warnings.simplefilter(\"ignore\")\r\n",
    "def cleanHtml(sentence):\r\n",
    "    cleanr = re.compile('<.*?>')\r\n",
    "    cleantext = re.sub(cleanr, ' ', str(sentence))\r\n",
    "    return cleantext\r\n",
    "def cleanPunc(sentence): #function to clean the word of any punctuation or special characters\r\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\r\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\r\n",
    "    cleaned = cleaned.strip()\r\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\r\n",
    "    return cleaned\r\n",
    "def keepAlpha(sentence):\r\n",
    "    alpha_sent = \"\"\r\n",
    "    for word in sentence.split():\r\n",
    "        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)\r\n",
    "        alpha_sent += alpha_word\r\n",
    "        alpha_sent += \" \"\r\n",
    "    alpha_sent = alpha_sent.strip()\r\n",
    "    return alpha_sent\r\n",
    "\r\n",
    "def textinitialpreprocess(def_string):\r\n",
    "    given_string = def_string.lower()\r\n",
    "    given_string = cleanHtml(given_string)\r\n",
    "    given_string = cleanPunc(given_string)\r\n",
    "    given_string = keepAlpha(given_string)\r\n",
    "    return given_string\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "window = tk.Tk()\r\n",
    "window.title(\"SCC Classifier\")\r\n",
    "window.geometry(\"1000x150\")\r\n",
    "copy_s = u\"\\u00A9\" \"Manish Puri\"\r\n",
    "window.configure(background=\"yellow\")\r\n",
    "title = tk.Label( text= \"This tool is developed to map input tweet or ordinances to SCC\", font=(\"Arial\", 10), background=\"yellow\")\r\n",
    "title.grid(column=1, row=0) \r\n",
    "label2 = tk.Label( text= \"Enter tweet/ordinance\", font=(\"Arial\", 10), background=\"yellow\")\r\n",
    "label2.grid(column=0, row=3)\r\n",
    "label4 = tk.Label( text= copy_s  , font=(\"Arial\", 10), background=\"yellow\")\r\n",
    "entry_field1= tk.Entry(width=120, bg = 'light gray', font = 'Calibri', fg = 'green', justify = 'center')\r\n",
    "entry_field1.grid(column=1,row=3)\r\n",
    "dict_phrases =  {'Smart Economy':0, 'Smart Mobility':0, 'Smart Environment':0, 'Smart Governance':0, 'Smart People':0, 'Smart Living':0, 'No Matches':0}\r\n",
    "\r\n",
    "\r\n",
    "def phrase_generator():   \r\n",
    "    economyWordCount    = 0  \r\n",
    "    environmentWordCount= 0\r\n",
    "    governanceWordCount = 0\r\n",
    "    peopleWordCount     = 0\r\n",
    "    mobilityWordCount   = 0 \r\n",
    "    livingWordCount     = 0   \r\n",
    "    file = open('SCC_KB.pkl', 'rb')\r\n",
    "    economy = pickle.load(file)\r\n",
    "    mobility = pickle.load(file)\r\n",
    "    environment = pickle.load(file)\r\n",
    "    governance = pickle.load(file)\r\n",
    "    people = pickle.load(file)\r\n",
    "    living = pickle.load(file)\r\n",
    "\r\n",
    "    counter = 0\r\n",
    "    arrayno=[]\r\n",
    "    name = str(entry_field1.get())\r\n",
    "    name = textinitialpreprocess(name)\r\n",
    "    name_without_stopwords = remove_stopwords(name)\r\n",
    "\r\n",
    "    for el in economy:\r\n",
    "        if el in name_without_stopwords or el in get_ekphrasis(name_without_stopwords) or el in spacy_lemmatizer(name_without_stopwords):\r\n",
    "            counter+=1\r\n",
    "            economyWordCount = economyWordCount + 1\r\n",
    "            dict_phrases['Smart Economy'] +=1\r\n",
    "            name_without_stopwords = name_without_stopwords.replace(el,'')            \r\n",
    "\r\n",
    "    for el in mobility:\r\n",
    "        if el in name_without_stopwords or el in get_ekphrasis(name_without_stopwords) or el in spacy_lemmatizer(name_without_stopwords):\r\n",
    "            counter+=1            \r\n",
    "            mobilityWordCount = mobilityWordCount + 1     \r\n",
    "            dict_phrases['Smart Mobility'] +=1  \r\n",
    "            name_without_stopwords = name_without_stopwords.replace(el,'')            \r\n",
    "\r\n",
    "    for el in environment:\r\n",
    "        if el in name_without_stopwords or el in get_ekphrasis(name_without_stopwords) or el in spacy_lemmatizer(name_without_stopwords):\r\n",
    "            counter+=1\r\n",
    "            environmentWordCount = environmentWordCount + 1     \r\n",
    "            dict_phrases['Smart Environment'] +=1  \r\n",
    "            name_without_stopwords = name_without_stopwords.replace(el,'') \r\n",
    "\r\n",
    "    for el in governance:\r\n",
    "        if el in name_without_stopwords or el in get_ekphrasis(name_without_stopwords) or el in spacy_lemmatizer(name_without_stopwords):\r\n",
    "            counter+=1\r\n",
    "            governanceWordCount = governanceWordCount + 1     \r\n",
    "            dict_phrases['Smart Governance'] +=1  \r\n",
    "            name_without_stopwords = name_without_stopwords.replace(el,'')\r\n",
    "\r\n",
    "    for el in living:\r\n",
    "        if el in name_without_stopwords or el in get_ekphrasis(name_without_stopwords) or el in spacy_lemmatizer(name_without_stopwords):\r\n",
    "            counter+=1\r\n",
    "            livingWordCount = livingWordCount + 1\r\n",
    "            dict_phrases['Smart Living'] +=1   \r\n",
    "            name_without_stopwords = name_without_stopwords.replace(el,'')\r\n",
    "\r\n",
    "    for el in people:\r\n",
    "        if el in name_without_stopwords or el in get_ekphrasis(name_without_stopwords) or el in spacy_lemmatizer(name_without_stopwords):\r\n",
    "            counter+=1\r\n",
    "            peopleWordCount = peopleWordCount + 1\r\n",
    "            dict_phrases['Smart People'] +=1   \r\n",
    "            name_without_stopwords = name_without_stopwords.replace(el,'')  \r\n",
    "    \r\n",
    "    if counter==0:\r\n",
    "        dict_phrases['No Matches'] +=1                         \r\n",
    "  \r\n",
    "    \r\n",
    "    if counter >0:\r\n",
    "        arrayno.append(counter)\r\n",
    "        for key in dict_phrases:\r\n",
    "            if dict_phrases[key] >0:\r\n",
    "                arrayno.append(key)\r\n",
    "                arrayno.append(dict_phrases[key])\r\n",
    "                \r\n",
    "                \r\n",
    "        for i in range(1,len(arrayno)):\r\n",
    "            if i %2==0:\r\n",
    "                arrayno[i]= '{:3.2f}'.format(arrayno[i]/counter*100)\r\n",
    "       \r\n",
    "        return arrayno\r\n",
    "    else:\r\n",
    "        arrayno.append(\"No Matches\")\r\n",
    "        return arrayno\r\n",
    "   \r\n",
    "\r\n",
    "def phrase_display():\r\n",
    "    greeting = phrase_generator()\r\n",
    "    \r\n",
    "    greeting_display= tk.Text(master=window,height =1, width=100,font=(\"Arial\", 10))\r\n",
    "    total = max(dict_phrases['Smart Economy'] + \\\r\n",
    "            dict_phrases['Smart Mobility'] + \\\r\n",
    "            dict_phrases['Smart Environment'] + \\\r\n",
    "            dict_phrases['Smart Governance'] + \\\r\n",
    "            dict_phrases['Smart People']+  \\\r\n",
    "            dict_phrases['Smart Living'],1)\r\n",
    "\r\n",
    "    Economy_per = \"{:.0%}\".format(dict_phrases['Smart Economy']/total)\r\n",
    "    Mobility_per = \"{:.0%}\".format(dict_phrases['Smart Mobility']/total)\r\n",
    "    Environment_per = \"{:.0%}\".format(dict_phrases['Smart Environment']/total)\r\n",
    "    Governance_per = \"{:.0%}\".format(dict_phrases['Smart Governance']/total)\r\n",
    "    People_per = \"{:.0%}\".format(dict_phrases['Smart People']/total)\r\n",
    "    Living_per = \"{:.0%}\".format(dict_phrases['Smart Living']/total)\r\n",
    "    \r\n",
    "    messagebox.showinfo(\"       Results of mapping\", \"Results:  \\n  \\\r\n",
    "                                Smart Economy: \\t \\t {} \\t {} \\n  \\\r\n",
    "                                Smart Mobility: \\t \\t {} \\t {} \\n  \\\r\n",
    "                                Smart Environment: \\t {} \\t {} \\n  \\\r\n",
    "                                Smart Governance: \\t {} \\t {} \\n  \\\r\n",
    "                                Smart People: \\t \\t {} \\t {} \\n  \\\r\n",
    "                                Smart Living: \\t \\t {} \\t {} \\n \".\r\n",
    "                                format(dict_phrases['Smart Economy'], Economy_per, \r\n",
    "                                        dict_phrases['Smart Mobility'], Mobility_per, \r\n",
    "                                        dict_phrases['Smart Environment'], Environment_per,\r\n",
    "                                        dict_phrases['Smart Governance'], Governance_per, \r\n",
    "                                        dict_phrases['Smart People'],People_per,\r\n",
    "                                        dict_phrases['Smart Living'], Living_per\r\n",
    "                                        \r\n",
    "                                        ))\r\n",
    "    total = 1\r\n",
    "    dict_phrases['Smart Economy'] = 0\r\n",
    "    dict_phrases['Smart Mobility'] =0\r\n",
    "    dict_phrases['Smart Environment'] =0\r\n",
    "    dict_phrases['Smart Governance'] =0\r\n",
    "    dict_phrases['Smart People'] = 0\r\n",
    "    dict_phrases['Smart Living'] = 0   \r\n",
    "\r\n",
    "button1 = tk.Button(text=\"Results \", command=phrase_display)\r\n",
    "button1.grid(column=1, row=4)\r\n",
    "window.mainloop()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0993227984ad8824c264444a38e824e4c951a7592d518f6e93ecdb2b05867cef"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}